[{"id":"e2e63f88-5a02-4bb3-b2f7-fce70eb20e85","userId":"4a344041-dcf0-40a0-ac4d-68029716f563","function":{"id":"openrouter_i_stream_response_ctl","name":"OpenRouterInterWithStreamResponse","meta":{"description":"openwebui integrates with Openrouter and perfectly supports streaming response output.","type":"pipe","manifest":{"title":"OpenRouter Integration With Stream Response For OpenWebUI","version":"0.4.1","description":"openwebui integrates with Openrouter and perfectly supports streaming response output.","author":"nutschan","license":"MIT"}},"content":"\"\"\"\ntitle: OpenRouter Integration With Stream Response For OpenWebUI\nversion: 0.4.1\ndescription: openwebui integrates with Openrouter and perfectly supports streaming response output.\nauthor: nutschan\nlicense: MIT\n\"\"\"\n\n\nimport re\nimport requests\nimport json\nimport traceback\nfrom typing import Optional, List, Union, Generator, Iterator\nfrom pydantic import BaseModel, Field\n\n\ndef _insert_citations(text: str, citations: list[str]) -> str:\n    if not citations or not text:\n        return text\n\n    pattern = r\"\\[(\\d+)\\]\"\n\n    def replace_citation(match_obj):\n        try:\n            num = int(match_obj.group(1))\n            if 1 <= num <= len(citations):\n                url = citations[num - 1]\n                return f\"[[{num}]]({url})\"\n            else:\n                return match_obj.group(0)\n        except (ValueError, IndexError):\n            return match_obj.group(0)\n\n    try:\n        return re.sub(pattern, replace_citation, text)\n    except Exception as e:\n        print(f\"Error during citation insertion: {e}\")\n        return text\n\n\ndef _format_citation_list(citations: list[str]) -> str:\n\n    if not citations:\n        return \"\"\n\n    try:\n        citation_list = [f\"{i+1}. {url}\" for i, url in enumerate(citations)]\n        return \"\\n\\n---\\nCitations:\\n\" + \"\\n\".join(citation_list)\n    except Exception as e:\n        print(f\"Error formatting citation list: {e}\")\n        return \"\"\n\n\n# --- Main Pipe class ---\nclass Pipe:\n    class Valves(BaseModel):\n        OPENROUTER_API_KEY: str = Field(\n            default=\"\", description=\"Your OpenRouter API key (required).\"\n        )\n        INCLUDE_REASONING: bool = Field(\n            default=True,\n            description=\"Request reasoning tokens from models that support it.\",\n        )\n        MODEL_PREFIX: Optional[str] = Field(\n            default=None,\n            description=\"Optional prefix for model names in Open WebUI (e.g., 'OR: ').\",\n        )\n        # NEW: Configurable request timeout\n        REQUEST_TIMEOUT: int = Field(\n            default=90,\n            description=\"Timeout for API requests in seconds.\",\n            gt=0,\n        )\n        MODEL_PROVIDERS: Optional[str] = Field(\n            default=None,\n            description=\"Comma-separated list of model providers to include or exclude. Leave empty to include all providers.\",\n        )\n        INVERT_PROVIDER_LIST: bool = Field(\n            default=False,\n            description=\"If true, the above 'Model Providers' list becomes an *exclude* list instead of an *include* list.\",\n        )\n        ENABLE_CACHE_CONTROL: bool = Field(\n            default=False,\n            description=\"Enable OpenRouter prompt caching by adding 'cache_control' to potentially large message parts. May reduce costs for supported models (e.g., Anthropic, Gemini) on subsequent calls with the same cached prefix. See OpenRouter docs for details.\",\n        )\n        FREE_ONLY: bool = Field(\n            default=False,\n            description=\"If true, only free models will be available.\",\n        )\n\n    def __init__(self):\n        self.type = \"manifold\"  # Specifies this pipe provides multiple models\n        self.valves = self.Valves()\n        if not self.valves.OPENROUTER_API_KEY:\n            print(\"Warning: OPENROUTER_API_KEY is not set in Valves.\")\n\n    def pipes(self) -> List[dict]:\n        \"\"\"\n        Fetches available models from the OpenRouter API.\n        This method is called by OpenWebUI to discover the models this pipe provides.\n        \"\"\"\n        if not self.valves.OPENROUTER_API_KEY:\n            return [\n                {\"id\": \"error\", \"name\": \"Pipe Error: OpenRouter API Key not provided\"}\n            ]\n\n        try:\n            headers = {\"Authorization\": f\"Bearer {self.valves.OPENROUTER_API_KEY}\"}\n            response = requests.get(\n                \"https://openrouter.ai/api/v1/models\",\n                headers=headers,\n                timeout=self.valves.REQUEST_TIMEOUT,\n            )\n            response.raise_for_status()\n\n            models_data = response.json()\n            raw_models_data = models_data.get(\"data\", [])\n            models: List[dict] = []\n\n            # --- Provider Filtering Logic ---\n            provider_list_str = (self.valves.MODEL_PROVIDERS or \"\").lower()\n            invert_list = self.valves.INVERT_PROVIDER_LIST\n            target_providers = {\n                p.strip() for p in provider_list_str.split(\",\") if p.strip()\n            }\n            # --- End Filtering Logic ---\n\n            for model in raw_models_data:\n                model_id = model.get(\"id\")\n                if not model_id:\n                    continue\n\n                # Apply Provider Filtering\n                if target_providers:\n                    provider = (\n                        model_id.split(\"/\", 1)[0].lower()\n                        if \"/\" in model_id\n                        else model_id.lower()\n                    )\n                    provider_in_list = provider in target_providers\n                    keep = (provider_in_list and not invert_list) or (\n                        not provider_in_list and invert_list\n                    )\n                    if not keep:\n                        continue\n\n                # Apply Free Only Filtering\n                if self.valves.FREE_ONLY and \"free\" not in model_id.lower():\n                    continue\n\n                model_name = model.get(\"name\", model_id)\n                prefix = self.valves.MODEL_PREFIX or \"\"\n                models.append({\"id\": model_id, \"name\": f\"{prefix}{model_name}\"})\n\n            if not models:\n                if self.valves.FREE_ONLY:\n                    return [{\"id\": \"error\", \"name\": \"Pipe Error: No free models found\"}]\n                elif target_providers:\n                    return [\n                        {\n                            \"id\": \"error\",\n                            \"name\": \"Pipe Error: No models found matching the provider filter\",\n                        }\n                    ]\n                else:\n                    return [\n                        {\n                            \"id\": \"error\",\n                            \"name\": \"Pipe Error: No models found on OpenRouter\",\n                        }\n                    ]\n\n            return models\n\n        except requests.exceptions.Timeout:\n            print(\"Error fetching models: Request timed out.\")\n            return [{\"id\": \"error\", \"name\": \"Pipe Error: Timeout fetching models\"}]\n        except requests.exceptions.HTTPError as e:\n            error_msg = f\"Pipe Error: HTTP {e.response.status_code} fetching models\"\n            try:\n                error_detail = e.response.json().get(\"error\", {}).get(\"message\", \"\")\n                if error_detail:\n                    error_msg += f\": {error_detail}\"\n            except json.JSONDecodeError:\n                pass\n            print(f\"Error fetching models: {error_msg} (URL: {e.request.url})\")\n            return [{\"id\": \"error\", \"name\": error_msg}]\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching models: Request failed: {e}\")\n            return [\n                {\n                    \"id\": \"error\",\n                    \"name\": f\"Pipe Error: Network error fetching models: {e}\",\n                }\n            ]\n        except Exception as e:\n            print(f\"Unexpected error fetching models: {e}\")\n            traceback.print_exc()\n            return [{\"id\": \"error\", \"name\": f\"Pipe Error: Unexpected error: {e}\"}]\n\n    def pipe(self, body: dict) -> Union[str, Generator, Iterator]:\n        \"\"\"\n        Processes incoming chat requests. This is the main function called by OpenWebUI\n        when a user interacts with a model provided by this pipe.\n\n        Args:\n            body: The request body, conforming to OpenAI chat completions format.\n\n        Returns:\n            Either a string (for non-streaming responses) or a generator/iterator\n            (for streaming responses).\n        \"\"\"\n        if not self.valves.OPENROUTER_API_KEY:\n            return \"Pipe Error: OpenRouter API Key is not configured.\"\n\n        try:\n            payload = body.copy()\n            if \"model\" in payload and payload[\"model\"] and \".\" in payload[\"model\"]:\n                payload[\"model\"] = payload[\"model\"].split(\".\", 1)[1]\n\n            # --- Apply Cache Control Logic ---\n            if self.valves.ENABLE_CACHE_CONTROL and \"messages\" in payload:\n                try:\n                    cache_applied = False\n                    messages = payload[\"messages\"]\n\n                    # 1. Try applying to System Message\n                    for i, msg in enumerate(messages):\n                        if msg.get(\"role\") == \"system\" and isinstance(\n                            msg.get(\"content\"), list\n                        ):\n                            longest_index, max_len = -1, -1\n                            for j, part in enumerate(msg[\"content\"]):\n                                if part.get(\"type\") == \"text\":\n                                    text_len = len(part.get(\"text\", \"\"))\n                                    if text_len > max_len:\n                                        max_len, longest_index = text_len, j\n                            if longest_index != -1:\n                                msg[\"content\"][longest_index][\"cache_control\"] = {\n                                    \"type\": \"ephemeral\"\n                                }\n                                cache_applied = True\n                                break\n\n                    # 2. Fallback to Last User Message\n                    if not cache_applied:\n                        for msg in reversed(messages):\n                            if msg.get(\"role\") == \"user\" and isinstance(\n                                msg.get(\"content\"), list\n                            ):\n                                longest_index, max_len = -1, -1\n                                for j, part in enumerate(msg[\"content\"]):\n                                    if part.get(\"type\") == \"text\":\n                                        text_len = len(part.get(\"text\", \"\"))\n                                        if text_len > max_len:\n                                            max_len, longest_index = text_len, j\n                                if longest_index != -1:\n                                    msg[\"content\"][longest_index][\"cache_control\"] = {\n                                        \"type\": \"ephemeral\"\n                                    }\n                                    break\n                except Exception as cache_err:\n                    print(f\"Warning: Error applying cache_control logic: {cache_err}\")\n                    traceback.print_exc()\n            # --- End Cache Control Logic ---\n\n            if self.valves.INCLUDE_REASONING:\n                payload[\"include_reasoning\"] = True\n\n            headers = {\n                \"Authorization\": f\"Bearer {self.valves.OPENROUTER_API_KEY}\",\n                \"Content-Type\": \"application/json\",\n                \"HTTP-Referer\": body.get(\"http_referer\", \"https://openwebui.com/\"),\n                \"X-Title\": body.get(\"x_title\", \"Open WebUI via Pipe\"),\n            }\n            url = \"https://openrouter.ai/api/v1/chat/completions\"\n            is_streaming = body.get(\"stream\", False)\n\n            if is_streaming:\n                return self.stream_response(\n                    url,\n                    headers,\n                    payload,\n                    _insert_citations,\n                    _format_citation_list,\n                    self.valves.REQUEST_TIMEOUT,\n                )\n            else:\n                return self.non_stream_response(\n                    url,\n                    headers,\n                    payload,\n                    _insert_citations,\n                    _format_citation_list,\n                    self.valves.REQUEST_TIMEOUT,\n                )\n\n        except Exception as e:\n            print(f\"Error preparing request in pipe method: {e}\")\n            traceback.print_exc()\n            return f\"Pipe Error: Failed to prepare request: {e}\"\n\n    def non_stream_response(\n        self, url, headers, payload, citation_inserter, citation_formatter, timeout\n    ) -> str:\n        \"\"\"Handles non-streaming API requests.\"\"\"\n        try:\n            response = requests.post(\n                url, headers=headers, json=payload, timeout=timeout\n            )\n            response.raise_for_status()\n\n            res = response.json()\n            if not res.get(\"choices\"):\n                return \"\"\n\n            choice = res[\"choices\"][0]\n            message = choice.get(\"message\", {})\n            citations = res.get(\"citations\", [])\n\n            content = message.get(\"content\", \"\")\n            reasoning = message.get(\"reasoning\", \"\")\n\n            content = citation_inserter(content, citations)\n            reasoning = citation_inserter(reasoning, citations)\n            citation_list = citation_formatter(citations)\n\n            final = \"\"\n            if reasoning:\n                final += f\"<think>\\n{reasoning}\\n</think>\\n\\n\"\n            if content:\n                final += content\n            if final:\n                final += citation_list\n            return final\n\n        except requests.exceptions.Timeout:\n            return f\"Pipe Error: Request timed out ({timeout}s)\"\n        except requests.exceptions.HTTPError as e:\n            error_msg = f\"Pipe Error: API returned HTTP {e.response.status_code}\"\n            try:\n                detail = e.response.json().get(\"error\", {}).get(\"message\", \"\")\n                if detail:\n                    error_msg += f\": {detail}\"\n            except Exception:\n                pass\n            return error_msg\n        except Exception as e:\n            print(f\"Unexpected error in non_stream_response: {e}\")\n            traceback.print_exc()\n            return f\"Pipe Error: Unexpected error processing response: {e}\"\n\n    def stream_response(\n        self, url, headers, payload, citation_inserter, citation_formatter, timeout\n    ) -> Generator[str, None, None]:\n        \"\"\"Handles streaming API requests using a generator.\"\"\"\n        response = None\n        try:\n            response = requests.post(\n                url, headers=headers, json=payload, stream=True, timeout=timeout\n            )\n            response.raise_for_status()\n\n            buffer = \"\"\n            in_think = False\n            latest_citations: List[str] = []\n\n            for line in response.iter_lines():\n                if not line or not line.startswith(b\"data: \"):\n                    continue\n                data = line[len(b\"data: \") :].decode(\"utf-8\")\n                if data == \"[DONE]\":\n                    break\n                try:\n                    chunk = json.loads(data)\n                except json.JSONDecodeError:\n                    continue\n\n                if \"choices\" in chunk:\n                    choice = chunk[\"choices\"][0]\n                    citations = chunk.get(\"citations\")\n                    if citations is not None:\n                        latest_citations = citations\n                    delta = choice.get(\"delta\", {})\n                    content = delta.get(\"content\", \"\")\n                    reasoning = delta.get(\"reasoning\", \"\")\n\n                    # reasoning\n                    if reasoning:\n                        if not in_think:\n                            if buffer:\n                                yield citation_inserter(buffer, latest_citations)\n                                buffer = \"\"\n                            yield \"<think>\\n\"\n                            in_think = True\n                        buffer += reasoning\n                        yield citation_inserter(buffer, latest_citations)\n                        buffer = \"\"\n\n                    # content\n                    if content:\n                        if in_think:\n                            if buffer:\n                                yield citation_inserter(buffer, latest_citations)\n                                buffer = \"\"\n                            yield \"\\n</think>\\n\\n\"\n                            in_think = False\n                        buffer += content\n                        yield citation_inserter(buffer, latest_citations)\n                        buffer = \"\"\n            # flush buffer\n            if buffer:\n                yield citation_inserter(buffer, latest_citations)\n            yield citation_formatter(latest_citations)\n\n        except requests.exceptions.Timeout:\n            yield f\"Pipe Error: Request timed out ({timeout}s)\"\n        except requests.exceptions.HTTPError as e:\n            yield f\"Pipe Error: API returned HTTP {e.response.status_code}\"\n        except Exception as e:\n            yield f\"Pipe Error: Unexpected error during streaming: {e}\"\n        finally:\n            if response:\n                response.close()\n"},"info":{"body":"OpenRouter Integration for OpenWebUI\nA simple integration pipe to connect OpenWebUI with OpenRouter, supporting model filtering, free model selection, and both streaming/non-streaming responses.\nFeatures\n● Model Filtering: Include/exclude models by provider (e.g., OpenAI, Anthropic).\n● Free Model Only: Optionally display only free models from OpenRouter.\n● Streaming Support: Real-time response streaming (if model supports it).\n● Citation Handling: Automatically format citations from OpenRouter's API.\n● Reasoning Tokens: Request and display model reasoning (for supported models)."},"downloads":17,"upvotes":0,"downvotes":0,"updatedAt":1753239705,"createdAt":1753239686,"user":{"id":"4a344041-dcf0-40a0-ac4d-68029716f563","username":"banburs","name":"","createdAt":1741425655,"role":null,"verified":false}}]