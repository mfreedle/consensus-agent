[
    {
        "id": "a316ff84-3efb-4dff-9cdd-0d5aaf308ac8",
        "userId": "3d339211-0f7d-45b8-8e83-4dfba202d8e0",
        "function": {
            "id": "openai_responses_api_pipeline",
            "name": "OpenAI Responses API Manifold",
            "meta": {
                "description": "Brings OpenAI Response API support to Open WebUI. Streams <think> reasoning (if enabled) and assistant text using the OpenAI Responses SDK, with early support for images, reasoning, and status indicators.",
                "type": "pipe",
                "manifest": {
                    "title": "OpenAI Responses API Pipeline",
                    "id": "openai_responses_api_pipeline",
                    "author": "Justin Kropp",
                    "author_url": "https://github.com/jrkropp",
                    "description": "Brings OpenAI Response API support to Open WebUI. Streams <think> reasoning (if enabled) and assistant text using the OpenAI Responses SDK, with early support for images, reasoning, and status indicators.",
                    "version": "1.6.2",
                    "license": "MIT",
                    "requirements": "openai>=1.76.0",
                    "Notes": ""
                }
            },
            "content": "\"\"\"\ntitle: OpenAI Responses API Pipeline\nid: openai_responses_api_pipeline\nauthor: Justin Kropp\nauthor_url: https://github.com/jrkropp\ndescription: Brings OpenAI Response API support to Open WebUI. Streams <think> reasoning (if enabled) and assistant text using the OpenAI Responses SDK, with early support for images, reasoning, and status indicators.\nversion: 1.6.2\nlicense: MIT\nrequirements: openai>=1.76.0\n\n-----------------------------------------------------------------------------------------\n📌 OVERVIEW\n-----------------------------------------------------------------------------------------\nThis pipeline integrates OpenAI Responses API with Open WebUI, enabling features exclusive\nto the Responses API. Simply drop it in to start using advanced models and capabilities.\n\nSupported features:\n   - 💬 Reasoning control via REASON_SUMMARY and REASON_EFFORT.\n   - 🖼️ Image input support (base64-encoded images via \"input_image\" parameter).\n   - 🔄 Streaming assistant responses and reasoning in real-time.\n   - 🌐 Web search integration (requires ENABLE_WEB_SEARCH=True).\n   - 🐞 Debug mode (DEBUG=True) for inspecting requests and streamed events.\n   - 📊 Usage statistics (tracks and displays token usage if enabled in the model).\n   - 🔧 Native tool calling (supports OpenAI function calling with OpenWebUI tools).\n   - 🛡️ Safety settings for tool usage and API calls.\n   - 🔑 Custom API keys (users can use their own API keys for requests).\n   - 📜 Citations (displays tool-generated citations for transparency and debugging).\n\nRoadmap / Future Improvements:\n   TODO 🖼️ Add support for latest OpenAI image model (gpt-image-1).  Enable via optional valve.  Ideally via tool calling and/or image button.\n   TODO 🔄 Add support for permanently saving tool outputs to the conversation history, not just passing them temporarily during the active request.\n   TODO 📄 Document input support (e.g., PDFs, other files via __files__ parameter in pipe() function).\n   TODO 📚 Enhanced RAG support (improved retrieval-augmented generation with better context injection).\n\nNotes:\n   - This pipeline is designed for OpenAI's Responses API and may not work with other APIs.\n   - This pipeline is still in development and may have bugs. Use at your own risk.\n   - To use multiple models, duplicate this pipeline and configure each copy for a specific model.\n   - Tool calling requires OpenWebUI model Advanced Params > Function Calling > set to \"Native\".\n\nRead more about the responses API:\n- https://openai.com/index/new-tools-for-building-agents/\n- https://platform.openai.com/docs/quickstart?api-mode=responses\n- https://platform.openai.com/docs/api-reference/responses\n\n-----------------------------------------------------------------------------------------\n🛠️ CHANGELOG\n-----------------------------------------------------------------------------------------\n• 1.6.2\n   - Fixed bug where it would check if the client is established each time a chunk is streamed.  Fixed by moving, 'client = get_openai_client(self.valves)' outside the while loop.\n\n• 1.6.1\n   - Updated requirements to \"openai>=1.76.0\" (library will automatically install when pipe in initialized).\n   - Added lazy and safe OpenAI client creation inside pipe() to avoid unnecessary re-instantiation.\n   - Cleaned up docstring for improved readability.\n\n• 1.6.0\n   - Added TOKEN_BUFFER_SIZE (default 1) for streaming control. This controls the number of tokens to buffer before yielding. Set to 1 for immediate per-token streaming.\n   - Cleaned up docstring at top of file for better readability.\n   - Refactored code for improved readability and maintainability.\n   - Rewrote transform_chat_messages_to_responses_api_format() for better readability and performance.\n   - Changed tool_choice behavior.  Now defaults to \"none\" if no tools are present.\n\n• 1.5.10\n   - Introduced True Parallel Tool Calling. Tool calls are now executed in parallel using asyncio.gather, then all results are appended at once before returning to the LLM. Previously, calls were handled one-by-one due to sequential loop logic.\n   - Set PARALLEL_TOOL_CALLS default back to True to match OpenAI's default behavior.\n   - The model now receives a clear system message when nearing the MAX_TOOL_CALLS limit, encouraging it to conclude tool use gracefully.\n   - Status messages now reflect when a tool is being invoked, with more personality and clarity for the user.\n   - Tool responses are now emitted as citations, giving visibility into raw results (especially useful for debugging and transparency).\n\n• 1.5.9\n  - Fixed bug where web_search tool could cause OpenAI responses to loop indefinitely.\n  - Introduced MAX_TOOL_CALLS valve (default 5) to limit the number of tool calls in a single request as extra safety precaution.\n  - Set PARALLEL_TOOL_CALLS valve default to False (prev. True).\n\n• 1.5.8\n  - Polished docstrings and streamlined debug logging output.\n  - Refactored code for improved readability.\n\n• 1.5.7\n  - Introduced native tool support for OpenAI! Integrate with OpenWebUI tools.\n\n• 1.5.6\n  - Fixed minor bugs in function calling and improved performance for large messages.\n  - Introduced partial support for multi-modal input.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport traceback\nimport asyncio\nimport json\nimport httpx\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Callable, Awaitable\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom fastapi import Request\n\n\nclass Pipe:\n    \"\"\"\n    A pipeline for streaming responses from the OpenAI Responses API.\n\n    The Responses API builds on the Chat Completions API by introducing server-side state,\n    built-in tools, and a rich event-based streaming format. This pipeline bridges the gap\n    between Open WebUI's Chat Completions-style input/output and the Responses API's\n    item-based schema.\n    \"\"\"\n\n    class Valves(BaseModel):\n        \"\"\"\n        Parameters controlling the API call and behavior.\n        \"\"\"\n\n        BASE_URL: Optional[str] = Field(\n            default=\"https://api.openai.com/v1\",\n            description=\"The base URL to use with the OpenAI SDK. Defaults to the official OpenAI API endpoint. Supports LiteLLM and other custom endpoints.\",\n        )\n\n        API_KEY: str = Field(\n            default=os.getenv(\"OPENAI_API_KEY\"),\n            description=(\n                \"Your OpenAI API key. If left blank, the environment variable \"\n                \"'OPENAI_API_KEY' will be used instead.\"\n            ),\n        )\n\n        MODEL_ID: str = Field(\n            default=\"gpt-4.1\",\n            description=(\n                \"Model ID used to generate responses. Defaults to 'gpt-4.1'. \"\n                \"Note: The model ID must be a valid OpenAI model ID. E.g. 'gpt-4o', 'o3', etc.\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/api-reference/responses/create#responses-create-model\n\n        REASON_SUMMARY: Optional[str] = Field(\n            default=None,\n            description=(\n                \"Reasoning summary style for o-series models (auto | concise | detailed)\"\n                \"Leave blank to skip passing 'reasoning' (default).\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/api-reference/responses/create#responses-create-reasoning\n\n        REASON_EFFORT: Optional[str] = Field(\n            default=None,\n            description=(\n                \"Reasoning effort level (low | medium | high). \"\n                \"Leave blank to skip passing 'reasoning' if not needed (default).\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/api-reference/responses/create#responses-create-reasoning\n\n        ENABLE_WEB_SEARCH: bool = Field(\n            default=False,\n            description=(\n                \"Whether to enable the built-in 'web_search' tool. \"\n                \"If True, adds {'type': 'web_search'} to tools (unless already present).\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses\n\n        SEARCH_CONTEXT_SIZE: Optional[str] = Field(\n            default=\"medium\",\n            description=(\n                \"Specifies the OpenAI web search context size: low | medium | high. \"\n                \"Default is 'medium'. Affects cost, quality, and latency\"\n                \"Only used if ENABLE_WEB_SEARCH=True.\"\n            ),\n        )\n        PARALLEL_TOOL_CALLS: bool = Field(\n            default=False,\n            description=(\n                \"Whether tool calls can be parallelized. Defaults to True if not set.\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/api-reference/responses/create#responses-create-parallel_tool_calls\n\n        MAX_TOOL_CALLS: int = Field(\n            default=5,\n            description=(\n                \"Maximum number of tool calls the model can make in a single request. \"\n                \"This is a hard stop safety limit to prevent infinite loops. \"\n                \"Defaults to 5.\"\n            ),\n        )\n        STORE_RESPONSE: bool = Field(\n            default=False,\n            description=(\n                \"Whether to store the generated model response (on OpenAI's side) for later debuging. \"\n                \"Defaults to False.\"\n            ),\n        )  # Read more: https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n\n        TOKEN_BUFFER_SIZE: int = Field(\n            default=1,\n            description=\"Number of partial tokens to buffer before yielding. \"\n            \"Set this to 1 (default) for immediate per-token streaming.\",\n        )\n\n        DEBUG: bool = Field(\n            default=False,\n            description=\"When True, prints debug statements to the console.  Do not leave this on in production.\",\n        )\n\n    def __init__(self):\n        print(\"Pipe __init__ called!\")\n        self.valves = self.Valves()\n        self.name = f\"OpenAI: {self.valves.MODEL_ID}\"\n\n    async def on_startup(self):\n        print(f\"on_startup:{__name__}\")\n\n    async def on_shutdown(self):\n        print(f\"on_shutdown:{__name__}\")\n\n    async def on_valves_updated(self):\n        print(f\"on_valves_updated: {__name__}\")\n\n    async def pipe(\n        self,\n        body: dict[str, Any],\n        __user__: Dict[str, Any],\n        __request__: Request,\n        __event_emitter__: Callable[[\"Event\"], Awaitable[None]],\n        __event_call__: Callable[[dict[str, Any]], Awaitable[Any]],\n        __task__: str,\n        __task_body__: dict[str, Any],\n        __files__: list[dict[str, Any]],\n        __metadata__: dict[str, Any],\n        __tools__: dict[str, Any],\n    ):\n        \"\"\"\n        Main pipeline method that:\n          1. Transforms incoming chat messages to OpenAI Responses format\n          2. Prepares tools (for function calling)\n          3. Builds a streaming request\n          4. Collects partial tokens in an array\n          5. Processes function calls (tools) if any, then loops if needed\n          6. Returns a final array of all text chunks\n        \"\"\"\n        \"\"\"\n        # Uncomments to show all function parameters when DEBUG is True (for exploration).  Commented out to avoid cluttering the console.\n        if self.valves.DEBUG:\n            print(\"---------------------------------------------------------\")\n            print(\"[DEBUG] Pipe function called with parameters:\")\n            all_params = {\n                \"body\": body,\n                \"__user__\": __user__,\n                \"__request__\": __request__,\n                \"__event_emitter__\": __event_emitter__,\n                \"__event_call__\": __event_call__,\n                \"__task__\": __task__,\n                \"__task_body__\": __task_body__,\n                \"__files__\": __files__,\n                \"__metadata__\": __metadata__,\n                \"__tools__\": __tools__,\n            }\n            print(json.dumps(all_params, indent=2, default=str))\n            print(\"---------------------------------------------------------\")\n        \"\"\"\n\n        # If tools are provided, but function calling is not 'native', warn the user\n        if __tools__ and __metadata__.get(\"function_calling\") != \"native\":\n            yield (\n                \"🛑 Tools detected, but native function calling is disabled.\\n\\n\"\n                'To enable tools in this chat, switch **Function Calling** to **\"Native\"** under:\\n'\n                \"⚙️ **Chat Controls** → **Advanced Params** → **Function Calling**\\n\\n\"\n                \"If you're an admin, you can also set this at the **model level**:\\n\"\n                \"**Model Settings** → **Advanced Params** → **Function Calling = Native**\"\n            )\n\n        # STEP 1: Establish OpenAI Client\n        client = get_openai_client(self.valves)\n\n        # STEP 2: Transform the user’s messages into the format the Responses API expects\n        all_messages = body.get(\"messages\", [])\n        transformed_messsage_array = transform_chat_messages_to_responses_api_format(\n            all_messages\n        )\n        input_messages = transformed_messsage_array[\"input\"]\n        instructions = transformed_messsage_array[\"instructions\"]\n\n        # STEP 3: Prepare any tools (function specs), if any\n        tools = prepare_tools(__tools__)\n        if self.valves.ENABLE_WEB_SEARCH:\n            tools.append(\n                {\n                    \"type\": \"web_search\",\n                    \"search_context_size\": self.valves.SEARCH_CONTEXT_SIZE,\n                }\n            )\n\n        # STEP 4: Build the request parameters\n        request_params = {\n            \"model\": self.valves.MODEL_ID,\n            \"tools\": tools,\n            \"tool_choice\": \"auto\" if tools else \"none\",\n            \"instructions\": instructions,\n            \"input\": input_messages,\n            \"parallel_tool_calls\": self.valves.PARALLEL_TOOL_CALLS,\n            \"max_output_tokens\": body.get(\"max_tokens\"),\n            \"temperature\": body.get(\"temperature\") or 1.0,\n            \"top_p\": body.get(\"top_p\") or 1.0,\n            \"user\": __user__.get(\"email\"),\n            \"store\": self.valves.STORE_RESPONSE,\n            \"text\": {\"format\": {\"type\": \"text\"}},\n            \"truncation\": \"auto\",\n            \"stream\": True,\n        }\n\n        if self.valves.REASON_EFFORT or self.valves.REASON_SUMMARY:\n            request_params[\"reasoning\"] = {}\n            if self.valves.REASON_EFFORT:\n                request_params[\"reasoning\"][\"effort\"] = self.valves.REASON_EFFORT\n            if self.valves.REASON_SUMMARY:\n                request_params[\"reasoning\"][\"summary\"] = self.valves.REASON_SUMMARY\n\n        tool_call_count = 0\n\n        # STEP 5: Loop until we either run out of tool calls or the conversation ends\n        while tool_call_count <= self.valves.MAX_TOOL_CALLS:\n            debug = self.valves.DEBUG\n            token_buffer_size = self.valves.TOKEN_BUFFER_SIZE\n\n            if debug:\n                print(\"[DEBUG] Calling OpenAI API with the following input_messages:\")\n                print(json.dumps(input_messages, indent=2, default=str))\n\n            # Try to call the OpenAI API and stream the response\n            try:\n                pending_function_calls = []\n                conversation_ended = False\n                response_stream = await client.responses.create(**request_params)\n\n                # Process each event in the response stream based on its type\n                is_model_thinking = False\n                buffer_parts = []  # Collect partial chunks here\n                event_count_since_flush = (\n                    0  # Count partial text events since last flush\n                )\n\n                async for event in response_stream:\n                    if debug:\n                        interesting_events = {\n                            \"response.function_call_arguments.done\",\n                            \"response.output_text.done\",\n                            \"response.failed\",\n                            \"response.done\",\n                            \"response.completed\",\n                            \"response.output_item.added\",\n                        }\n                        print(f\"\\033[1;92m[DEBUG] Event type: {event.type}\\033[0m\")\n                        if event.type in interesting_events:\n                            print(\n                                f\"\\033[1;93m{json.dumps(event.__dict__, indent=2, default=str)}\\033[0m\"\n                            )\n\n                    event_type = event.type\n                    chunk = \"\"\n\n                    # -------------------------------------------------------------------\n                    # REASONING (chain-of-thought) partial events\n                    # -------------------------------------------------------------------\n                    if event_type == \"response.reasoning_summary_text.delta\":\n                        if not is_model_thinking:\n                            chunk += \"<think>\"  # OpenWebUI expects <think> tags to be used for reasoning\n                            is_model_thinking = True\n                        chunk += event.delta\n\n                    elif event_type == \"response.reasoning_summary_text.done\":\n                        chunk += \"</think>\\n\"\n                        is_model_thinking = False\n\n                    # -------------------------------------------------------------------\n                    # NORMAL TEXT (assistant output) partial events\n                    # -------------------------------------------------------------------\n                    elif event_type == \"response.output_text.delta\":\n                        # If we were in a <think> block, close it first\n                        if is_model_thinking:\n                            buffer_parts.append(\"</think>\\n\")\n                            is_model_thinking = False\n                            event_count_since_flush += 1\n                        chunk += event.delta\n\n                    elif event_type == \"response.output_text.done\":\n                        chunk += \"\\n\\n\"\n\n                    # -------------------------------------------------------------------\n                    # FUNCTION CALLS OR STATUS\n                    # -------------------------------------------------------------------\n                    elif event_type == \"response.output_item.added\":\n                        item = getattr(event, \"item\", None)\n                        if item and item.type == \"function_call\" and __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\n                                        \"description\": f\"🔧 Hmm, let me run my {item.name} tool real quick...\",\n                                        \"done\": False,\n                                    },\n                                }\n                            )\n                        continue\n\n                    elif event_type == \"response.output_item.done\":\n                        item = getattr(event, \"item\", None)\n                        if item.type == \"function_call\":\n                            # queue the function call for processing\n                            pending_function_calls.append(item)\n                        continue\n\n                    elif event_type == \"response.content_part.added\":\n                        if __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\"description\": \"\", \"done\": True},\n                                }\n                            )\n                        continue\n\n                    elif event_type == \"response.web_search_call.in_progress\":\n                        if __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\n                                        \"description\": f\"🔍 Searching the internet...\",\n                                        \"done\": False,\n                                    },\n                                }\n                            )\n                        continue\n\n                    # -------------------------------------------------------------------\n                    # CITATIONS / ANNOTATIONS\n                    # -------------------------------------------------------------------\n                    elif event_type == \"response.output_text.annotation.added\":\n                        raw_anno = str(getattr(event, \"annotation\", \"\"))\n                        title_m = re.search(r\"title='([^']*)'\", raw_anno)\n                        url_m = re.search(r\"url='([^']*)'\", raw_anno)\n                        title = title_m.group(1) if title_m else \"Unknown Title\"\n                        url = url_m.group(1) if url_m else \"\"\n                        url = url.replace(\"?utm_source=openai\", \"\").replace(\n                            \"&utm_source=openai\", \"\"\n                        )\n                        if __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"citation\",\n                                    \"data\": {\n                                        \"document\": [title],\n                                        \"metadata\": [\n                                            {\n                                                \"date_accessed\": datetime.now().isoformat(),\n                                                \"source\": title,\n                                            }\n                                        ],\n                                        \"source\": {\"name\": url, \"url\": url},\n                                    },\n                                }\n                            )\n                        continue\n\n                    # -------------------------------------------------------------------\n                    # END OF RESPONSE OR ERROR\n                    # -------------------------------------------------------------------\n                    elif event_type in (\n                        \"response.completed\",\n                        \"response.done\",\n                        \"response.incomplete\",\n                        \"response.failed\",\n                    ):\n                        conversation_ended = True\n\n                    elif event_type == \"error\":\n                        conversation_ended = True\n\n                    else:\n                        # Unhandled event type\n                        continue\n\n                    # -------------------------------------------------------------------\n                    # 2) Append any new chunk to the buffer\n                    # -------------------------------------------------------------------\n                    if chunk:\n                        buffer_parts.append(chunk)\n                        event_count_since_flush += 1\n\n                    # -------------------------------------------------------------------\n                    # 3) Check if we should flush\n                    # -------------------------------------------------------------------\n                    if (\n                        event_count_since_flush >= token_buffer_size\n                        or conversation_ended\n                    ):\n                        yield \"\".join(buffer_parts)\n                        buffer_parts.clear()\n                        event_count_since_flush = 0\n\n                        if conversation_ended:\n                            break\n\n                # 4) Final flush after loop ends, in case there's leftover text\n                if buffer_parts:\n                    yield \"\".join(buffer_parts)\n                    buffer_parts.clear()\n\n            except Exception as ex:\n                # On error, yield it and break\n                yield f\"❌ {type(ex).__name__}: {ex}\\n{''.join(traceback.format_exc(limit=5))}\"\n                break\n\n            # ----------------------------------------------------------------\n            # 3) If we found any function calls, call them, append the results to bottom of conversation, and re-loop\n            # ----------------------------------------------------------------\n            if pending_function_calls:\n                tool_call_count += 1\n\n                # Step A: For each pending function call, add a \"function_call\" item,\n                # and prepare the async tasks for the actual tool calls.\n                tasks = []\n                for fc_item in pending_function_calls:\n                    # Record the function_call in the conversation\n                    input_messages.append(\n                        {\n                            \"type\": \"function_call\",\n                            \"call_id\": fc_item.call_id,\n                            \"name\": fc_item.name,\n                            \"arguments\": fc_item.arguments,\n                        }\n                    )\n\n                    # Look up and queue the tool callable\n                    tool = __tools__.get(fc_item.name)\n                    if tool is None:\n                        continue\n                    try:\n                        args = json.loads(fc_item.arguments or \"{}\")\n                    except:\n                        args = {}\n                    tasks.append(asyncio.create_task(tool[\"callable\"](**args)))\n\n                # Step B: Collect the results of all tool calls\n                try:\n                    results = await asyncio.gather(*tasks)\n                except Exception as ex:\n                    results = [f\"Error: {ex}\"] * len(tasks)\n\n                # Step C: Append the \"function_call_output\" items\n                for fc_item, tool_result in zip(pending_function_calls, results):\n                    input_messages.append(\n                        {\n                            \"type\": \"function_call_output\",\n                            \"call_id\": fc_item.call_id,\n                            \"output\": str(tool_result),\n                        }\n                    )\n\n                    # Optionally emit a \"citation\" so user sees the tool's raw output\n                    if __event_emitter__:\n                        await __event_emitter__(\n                            {\n                                \"type\": \"citation\",\n                                \"data\": {\n                                    \"document\": [\n                                        f\"{fc_item.name.title()} Tool Output\\n\\n{tool_result}\"\n                                    ],\n                                    \"metadata\": [\n                                        {\n                                            \"date_accessed\": datetime.now().isoformat(),\n                                            \"source\": fc_item.name,\n                                        }\n                                    ],\n                                    \"source\": {\"name\": f\"{fc_item.name.title()} Tool\"},\n                                },\n                            }\n                        )\n\n                # Step D: If near the limit, warn\n                if tool_call_count == self.valves.MAX_TOOL_CALLS - 1:\n                    input_messages.append(\n                        {\n                            \"role\": \"system\",\n                            \"content\": (\n                                f\"[ToolCallLimitApproaching] You have used {tool_call_count} \"\n                                f\"of {self.valves.MAX_TOOL_CALLS} tool calls in this response. \"\n                                \"**Exactly one call** remains before the quota is exhausted.\\n\\n\"\n                                \"• If that final call is essential, invoke it now.\\n\"\n                                \"• After this, we'll automatically set `tool_choice='none'`.\\n\\n\"\n                                \"You can always ask **continue** (or similar) to start fresh \"\n                                \"with a new tool-call quota.\"\n                            ),\n                        }\n                    )\n\n                # Step E: If we've hit or exceeded max calls, disable further tool use\n                if tool_call_count >= self.valves.MAX_TOOL_CALLS:\n                    request_params[\"tool_choice\"] = \"none\"\n\n                # Re-loop now that your conversation includes function_call + outputs\n                continue\n\n            # ----------------------------------------------------------------\n            # 4) If no function calls or conversation ended, exit\n            # ----------------------------------------------------------------\n            if conversation_ended or not pending_function_calls:\n                break\n\n\n###############################################################################\n# Module-level Helper Functions (Outside Pipe Class)\n###############################################################################\ndef get_openai_client(valves):\n    \"\"\"\n    Lazy-create and cache a shared AsyncOpenAI client.\n    Rebuilds only if API key or base URL change.\n    Logs debug information if valves.DEBUG is True.\n    \"\"\"\n    cached = getattr(get_openai_client, \"_client\", None)\n    cfg = getattr(get_openai_client, \"_cfg\", None)\n    new_cfg = (valves.API_KEY, valves.BASE_URL)\n\n    if cached is None:\n        if valves.DEBUG:\n            print(\"[DEBUG] No OpenAI client cached yet — creating new one...\")\n    elif cfg != new_cfg:\n        if valves.DEBUG:\n            print(\n                f\"[DEBUG] Config changed! Old={cfg} New={new_cfg} — rebuilding OpenAI client...\"\n            )\n    else:\n        if valves.DEBUG:\n            print(\"[DEBUG] Reusing cached OpenAI client.\")\n\n    if cached is None or cfg != new_cfg:\n        transport = httpx.AsyncClient(http2=True, timeout=90)\n        cached = AsyncOpenAI(\n            api_key=valves.API_KEY,\n            base_url=valves.BASE_URL,\n            http_client=transport,\n        )\n        get_openai_client._client = cached\n        get_openai_client._cfg = new_cfg\n\n        if valves.DEBUG:\n            print(\"[DEBUG] OpenAI client initialized (HTTP/2 enabled)\")\n\n    return cached\n\n\ndef prepare_tools(__tools__: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Converts user-provided tools into the standard format the OpenAI API expects.\n\n    Example of __tools__ input:\n      {\n        \"websearch\": {\n          \"spec\": {\n            \"name\": \"websearch\",\n            \"description\": \"...\",\n            \"parameters\": {...}\n          },\n          \"callable\": <async function>\n        },\n        \"calculator\": {\n          \"spec\": {\n            \"name\": \"calculator\",\n            \"description\": \"...\",\n            \"parameters\": {...}\n          },\n          \"callable\": <async function>\n        }\n      }\n    \"\"\"\n    tools = []\n\n    if __tools__:\n        for tool_name, tool_info in __tools__.items():\n            # Each tool has a \"spec\" dict describing its schema\n            spec = tool_info.get(\"spec\", {})\n            # Force the type to \"function\" to match OpenAI's function calling\n            spec[\"type\"] = \"function\"\n            tools.append(spec)\n\n    return tools\n\n\ndef transform_chat_messages_to_responses_api_format(messages):\n    \"\"\"\n    Convert WebUI Chat-Completions history → OpenAI Responses format.\n\n    INPUT EXAMPLE\n        [\n            {\"role\": \"system\",    \"content\": \"You are helpful.\"},\n            {\"role\": \"user\",      \"content\": \"Hi!\"},\n            {\"role\": \"assistant\", \"content\": \"Hello 👋\"},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What’s in this picture?\"},\n                    {\"type\": \"image_url\",\n                     \"image_url\": {\"url\": \"data:image/png;base64,AAA…\"}}\n                ]\n            }\n        ]\n\n    OUTPUT EXAMPLE\n        {\n            \"instructions\": \"You are helpful.\",\n            \"input\": [\n                {\"role\": \"user\",\n                 \"content\": [{\"type\": \"input_text\",\n                              \"text\": \"Hi!\"}]},\n                {\"role\": \"assistant\",\n                 \"content\": [{\"type\": \"output_text\",\n                              \"text\": \"Hello 👋\"}]},\n                {\"role\": \"user\",\n                 \"content\": [\n                     {\"type\": \"input_text\",\n                      \"text\": \"What’s in this picture?\"},\n                     {\"type\": \"input_image\",\n                      \"image_url\": \"data:image/png;base64,AAA…\"}\n                 ]}\n            ]\n        }\n    \"\"\"\n    instructions = None\n    output = []\n\n    # ── system → instructions ───────────────────────────────────────────────\n    if messages and messages[0].get(\"role\") == \"system\":\n        instructions = str(messages[0].get(\"content\", \"\")).strip()\n        messages = messages[1:]\n\n    # ── convert remaining messages ──────────────────────────────────────────\n    for msg in messages:\n        role = msg.get(\"role\", \"user\")\n        is_assistant = role == \"assistant\"\n\n        items = msg.get(\"content\", [])\n        if not isinstance(items, list):\n            items = [items]\n\n        converted = []\n        for item in items:\n            if item is None:  # guard against nulls\n                continue\n\n            # A) structured dict items\n            if isinstance(item, dict):\n                itype = item.get(\"type\", \"text\")\n\n                if is_assistant:\n                    if itype == \"refusal\":\n                        converted.append(\n                            {\n                                \"type\": \"refusal\",\n                                \"reason\": item.get(\"reason\", \"No reason\"),\n                            }\n                        )\n                    else:  # output text\n                        converted.append(\n                            {\"type\": \"output_text\", \"text\": item.get(\"text\", \"\")}\n                        )\n                else:  # user\n                    if itype == \"image_url\":\n                        url = item.get(\"image_url\", {}).get(\"url\", \"\")\n                        converted.append({\"type\": \"input_image\", \"image_url\": url})\n                    else:  # input text\n                        converted.append(\n                            {\"type\": \"input_text\", \"text\": item.get(\"text\", \"\")}\n                        )\n            # B) primitive str / int items\n            else:\n                text_val = item if isinstance(item, str) else str(item)\n                converted.append(\n                    {\n                        \"type\": \"output_text\" if is_assistant else \"input_text\",\n                        \"text\": text_val,\n                    }\n                )\n\n        output.append({\"role\": role, \"content\": converted})\n\n    return {\"instructions\": instructions, \"input\": output}\n"
        },
        "info": {
            "body": "⚠️THIS FUNCTION REPLACED⚠️\nUpdated link:\nhttps://github.com/jrkropp/open-webui-developer-toolkit/tree/main/functions/pipes/openai_responses_manifold"
        },
        "downloads": 344,
        "upvotes": 0,
        "downvotes": 0,
        "updatedAt": 1750788734,
        "createdAt": 1745038520,
        "user": {
            "id": "3d339211-0f7d-45b8-8e83-4dfba202d8e0",
            "username": "jkropp",
            "name": "",
            "createdAt": 1740180757,
            "role": null,
            "verified": false
        }
    }
]