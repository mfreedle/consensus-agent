[
  {
    "id": "fa8d99ba-eb25-4882-9a51-647adfb5dfe2",
    "userId": "34ef1096-ef7f-40f7-993d-dbc9bdcab631",
    "function": {
      "id": "o3pro_o1pro_support",
      "name": "O3 Pro O1 Pro Support",
      "meta": {
        "description": "OpenAI o1-pro & o3-pro Integration for Open Web UI",
        "type": "pipe",
        "manifest": {
          "Author": "Karan Bansal (@karanb192)",
          "License": "MIT License"
        }
      },
      "content": "\"\"\"\nAuthor: Karan Bansal (@karanb192)\nLicense: MIT License\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nimport httpx\nfrom typing import Dict, List, Optional, Any\nimport logging\nimport hashlib\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        NAME_PREFIX: str = Field(\n            default=\"OpenAI: \",\n            description=\"Prefix to be added before model names.\",\n        )\n        BASE_URL: str = Field(\n            default=\"https://api.openai.com/v1\",\n            description=\"Base URL for OpenAI API.\",\n        )\n        API_KEYS: str = Field(\n            default=\"\",\n            description=\"API keys for OpenAI, use comma to separate multiple keys\",\n        )\n        THINKING_EFFORT: str = Field(\n            default=\"medium\",\n            description=\"Reasoning effort: low, medium, high\",\n        )\n        SHOW_TOKEN_STATS: bool = Field(\n            default=True,\n            description=\"Display token usage statistics with each response\",\n        )\n        SHOW_CUMULATIVE_COST: bool = Field(\n            default=True,\n            description=\"Show cumulative cost for the entire conversation\",\n        )\n        MAX_OUTPUT_TOKENS: int = Field(\n            default=3200,\n            description=\"Maximum number of output tokens (1-32768)\",\n        )\n        TIMEOUT_SECONDS: int = Field(\n            default=600,\n            description=\"Request timeout in seconds\",\n        )\n        DEBUG_MODE: bool = Field(\n            default=False,\n            description=\"Enable debug logging to see raw API responses\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.openai_response_models = [\"o1-pro\", \"o3-pro\"]\n        self.api_key_index = 0\n        self.logger = self._setup_logger()\n        # Store conversation costs per user/conversation\n        self.conversation_costs = {}\n\n    def _setup_logger(self):\n        \"\"\"Setup logging for debugging\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n        return logger\n\n    def _get_conversation_id(self, body: dict, user: dict) -> str:\n        \"\"\"Generate a unique conversation identifier based on the FIRST user message only\"\"\"\n        # Get the first user message to create a stable conversation ID\n        first_user_message = None\n\n        for msg in body.get(\"messages\", []):\n            if msg.get(\"role\") == \"user\":\n                content = msg.get(\"content\", \"\")\n                if isinstance(content, list):\n                    # Handle multi-modal content\n                    text_content = \" \".join(\n                        [\n                            item.get(\"text\", \"\")\n                            for item in content\n                            if item.get(\"type\") == \"text\"\n                        ]\n                    )\n                    first_user_message = text_content\n                else:\n                    first_user_message = str(content)\n                break  # Only use the FIRST user message\n\n        if not first_user_message:\n            first_user_message = \"no_message\"\n\n        # Create a hash based on user ID and first message only\n        user_id = user.get(\"id\", \"unknown\")\n\n        # Use first 50 chars of first message for uniqueness\n        message_key = first_user_message[:50]\n\n        # Create stable conversation ID\n        conv_hash = hashlib.sha256(f\"{user_id}_{message_key}\".encode()).hexdigest()[:16]\n        conv_id = f\"conv_{user_id}_{conv_hash}\"\n\n        if self.valves.DEBUG_MODE:\n            user_msg_count = len(\n                [m for m in body.get(\"messages\", []) if m.get(\"role\") == \"user\"]\n            )\n            self.logger.info(\n                f\"Conversation ID: {conv_id}, User messages in history: {user_msg_count}\"\n            )\n            self.logger.info(f\"First message preview: {message_key[:30]}...\")\n\n        return conv_id\n\n    def _update_conversation_cost(self, conv_id: str, cost: float, tokens: Dict):\n        \"\"\"Update cumulative cost for a conversation\"\"\"\n        if conv_id not in self.conversation_costs:\n            self.conversation_costs[conv_id] = {\n                \"total_cost\": 0.0,\n                \"total_input_tokens\": 0,\n                \"total_output_tokens\": 0,\n                \"total_reasoning_tokens\": 0,\n                \"message_count\": 0,\n            }\n\n        # Add to existing totals (not replace)\n        self.conversation_costs[conv_id][\"total_cost\"] += cost\n        self.conversation_costs[conv_id][\"total_input_tokens\"] += tokens.get(\"input\", 0)\n        self.conversation_costs[conv_id][\"total_output_tokens\"] += tokens.get(\n            \"output\", 0\n        )\n        self.conversation_costs[conv_id][\"total_reasoning_tokens\"] += tokens.get(\n            \"reasoning\", 0\n        )\n        self.conversation_costs[conv_id][\"message_count\"] += 1\n\n        if self.valves.DEBUG_MODE:\n            self.logger.info(\n                f\"Updated conversation {conv_id}: Total cost now ${self.conversation_costs[conv_id]['total_cost']:.4f}\"\n            )\n\n    def _get_next_api_key(self) -> str:\n        \"\"\"Round-robin API key selection\"\"\"\n        keys = [k.strip() for k in self.valves.API_KEYS.split(\",\") if k.strip()]\n        if not keys:\n            raise ValueError(\"No API keys configured\")\n\n        key = keys[self.api_key_index % len(keys)]\n        self.api_key_index += 1\n        return key\n\n    def pipes(self):\n        \"\"\"Return available models\"\"\"\n        res = []\n        for model in self.openai_response_models:\n            res.append({\"name\": f\"{self.valves.NAME_PREFIX}{model}\", \"id\": model})\n        return res\n\n    def _transform_messages(self, messages: List[Dict]) -> List[Dict]:\n        \"\"\"Transform Open Web UI messages to Responses API format\"\"\"\n        new_messages = []\n\n        for message in messages:\n            try:\n                role = message.get(\"role\")\n                content = message.get(\"content\")\n\n                if role == \"user\":\n                    if isinstance(content, list):\n                        transformed_content = []\n                        for item in content:\n                            if item[\"type\"] == \"text\":\n                                transformed_content.append(\n                                    {\"type\": \"input_text\", \"text\": item[\"text\"]}\n                                )\n                            elif item[\"type\"] == \"image_url\":\n                                transformed_content.append(\n                                    {\n                                        \"type\": \"input_image\",\n                                        \"image_url\": item[\"image_url\"][\"url\"],\n                                    }\n                                )\n                        new_messages.append(\n                            {\"role\": \"user\", \"content\": transformed_content}\n                        )\n                    else:\n                        new_messages.append(\n                            {\n                                \"role\": \"user\",\n                                \"content\": [{\"type\": \"input_text\", \"text\": content}],\n                            }\n                        )\n\n                elif role == \"assistant\":\n                    new_messages.append(\n                        {\n                            \"role\": \"assistant\",\n                            \"content\": [{\"type\": \"output_text\", \"text\": content}],\n                        }\n                    )\n\n                elif role == \"system\":\n                    new_messages.append(\n                        {\n                            \"role\": \"system\",\n                            \"content\": [{\"type\": \"input_text\", \"text\": content}],\n                        }\n                    )\n\n            except Exception as e:\n                self.logger.error(f\"Message transformation error: {e}\")\n                raise ValueError(f\"Invalid message format: {message}\")\n\n        return new_messages\n\n    def _extract_text_from_output(self, output_items: List[Any]) -> str:\n        \"\"\"Extract text from output array\"\"\"\n        text_parts = []\n\n        for item in output_items:\n            if isinstance(item, dict):\n                # Check if it's a message type\n                if item.get(\"type\") == \"message\" and item.get(\"role\") == \"assistant\":\n                    content_items = item.get(\"content\", [])\n                    for content in content_items:\n                        if (\n                            isinstance(content, dict)\n                            and content.get(\"type\") == \"output_text\"\n                        ):\n                            text = content.get(\"text\", \"\")\n                            if text:\n                                text_parts.append(text)\n                # Check for direct text content\n                elif \"text\" in item:\n                    text_parts.append(item[\"text\"])\n                elif \"content\" in item:\n                    if isinstance(item[\"content\"], str):\n                        text_parts.append(item[\"content\"])\n                    elif isinstance(item[\"content\"], list):\n                        for content in item[\"content\"]:\n                            if isinstance(content, dict) and \"text\" in content:\n                                text_parts.append(content[\"text\"])\n            elif isinstance(item, str):\n                text_parts.append(item)\n\n        return \"\\n\".join(text_parts)\n\n    def _format_token_stats(\n        self,\n        usage: Dict,\n        model_id: str,\n        status: str = \"completed\",\n        incomplete_reason: str = None,\n        conv_id: str = None,\n        current_cost: float = 0.0,\n    ) -> str:\n        \"\"\"Format token usage statistics for display\"\"\"\n        if not usage or not self.valves.SHOW_TOKEN_STATS:\n            return \"\"\n\n        stats = f\"\\n\\n---\\nüìä **Token Usage (This Message)**:\\n\"\n\n        # Extract token counts\n        input_tokens = usage.get(\"input_tokens\", 0)\n        output_tokens = usage.get(\"output_tokens\", 0)\n        total_tokens = usage.get(\"total_tokens\", 0)\n\n        # Check for reasoning tokens in output_tokens_details\n        reasoning_tokens = 0\n        if \"output_tokens_details\" in usage:\n            reasoning_tokens = usage[\"output_tokens_details\"].get(\"reasoning_tokens\", 0)\n\n        stats += f\"- Input: {input_tokens:,} tokens\\n\"\n\n        if reasoning_tokens > 0:\n            stats += f\"- Reasoning: {reasoning_tokens:,} tokens\\n\"\n\n        stats += f\"- Output: {output_tokens:,} tokens\\n\"\n        stats += f\"- Total: {total_tokens:,} tokens\\n\"\n        stats += f\"- **Cost for this message**: ${current_cost:.4f}\\n\"\n\n        # Add cumulative stats if enabled\n        if (\n            self.valves.SHOW_CUMULATIVE_COST\n            and conv_id\n            and conv_id in self.conversation_costs\n        ):\n            conv_stats = self.conversation_costs[conv_id]\n            stats += f\"\\nüí∞ **Conversation Totals**:\\n\"\n            stats += f\"- Messages: {conv_stats['message_count']}\\n\"\n            stats += f\"- Total Input: {conv_stats['total_input_tokens']:,} tokens\\n\"\n            if conv_stats[\"total_reasoning_tokens\"] > 0:\n                stats += f\"- Total Reasoning: {conv_stats['total_reasoning_tokens']:,} tokens\\n\"\n            stats += f\"- Total Output: {conv_stats['total_output_tokens']:,} tokens\\n\"\n            stats += f\"- **Total Cost**: ${conv_stats['total_cost']:.4f}\\n\"\n\n        # Add status information\n        if status == \"incomplete\":\n            stats += f\"\\n‚ö†Ô∏è **Response Status**: Incomplete\"\n            if incomplete_reason:\n                stats += f\" (Reason: {incomplete_reason})\"\n            stats += \"\\n\"\n\n        stats += \"---\"\n        return stats\n\n    async def pipe(self, body: dict, __user__: dict):\n        \"\"\"Main pipeline function\"\"\"\n        try:\n            # Get conversation ID (stable across messages)\n            conv_id = self._get_conversation_id(body, __user__)\n\n            # Get API key\n            api_key = self._get_next_api_key()\n\n            # Setup headers\n            headers = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\",\n            }\n\n            # Extract model ID\n            model_id = body[\"model\"][body[\"model\"].find(\".\") + 1 :]\n\n            if model_id not in self.openai_response_models:\n                yield f\"Error: Model {model_id} not supported. Only o1-pro and o3-pro are available.\"\n                return\n\n            # Build payload (non-streaming for accurate token counts)\n            payload = {\n                \"model\": model_id,\n                \"reasoning\": {\"effort\": self.valves.THINKING_EFFORT.strip()},\n                \"max_output_tokens\": min(max(self.valves.MAX_OUTPUT_TOKENS, 1), 32768),\n            }\n\n            # Transform messages\n            payload[\"input\"] = self._transform_messages(body[\"messages\"])\n\n            # Get the actual message count from conversation tracking\n            current_message_number = 1\n            if conv_id in self.conversation_costs:\n                current_message_number = (\n                    self.conversation_costs[conv_id][\"message_count\"] + 1\n                )\n\n            # Show processing message\n            yield f\"üîÑ Processing with {model_id} (Message #{current_message_number} in conversation)...\\n\\n\"\n\n            async with httpx.AsyncClient(timeout=self.valves.TIMEOUT_SECONDS) as client:\n                # Make non-streaming request\n                response = await client.post(\n                    f\"{self.valves.BASE_URL}/responses\",\n                    json=payload,\n                    headers=headers,\n                )\n\n                if response.status_code != 200:\n                    error_text = response.text\n                    yield f\"Error: {response.status_code} {error_text}\"\n                    return\n\n                # Parse the complete response\n                response_data = response.json()\n\n                # Debug log if enabled\n                if self.valves.DEBUG_MODE:\n                    self.logger.info(f\"Response keys: {list(response_data.keys())}\")\n\n                # Extract the output text - try multiple methods\n                output_text = None\n\n                # Method 1: Direct output_text field\n                if \"output_text\" in response_data and response_data[\"output_text\"]:\n                    output_text = response_data[\"output_text\"]\n\n                # Method 2: Parse from output array\n                if not output_text and \"output\" in response_data:\n                    output_text = self._extract_text_from_output(\n                        response_data[\"output\"]\n                    )\n\n                # Method 3: Check in other possible locations\n                if not output_text:\n                    # Try extracting from string representation of output\n                    output_str = str(response_data.get(\"output\", \"\"))\n                    if \"text='\" in output_str or 'text=\"' in output_str:\n                        # Extract text between quotes\n                        import re\n\n                        matches = re.findall(r\"text=['\\\"]([^'\\\"]+)['\\\"]\", output_str)\n                        if matches:\n                            output_text = \"\\n\".join(matches)\n\n                if output_text:\n                    yield output_text\n                else:\n                    yield \"No response text found in the API response.\"\n\n                # Check response status\n                status = response_data.get(\"status\", \"completed\")\n                incomplete_reason = None\n                if status == \"incomplete\":\n                    incomplete_details = response_data.get(\"incomplete_details\", {})\n                    incomplete_reason = incomplete_details.get(\"reason\", \"Unknown\")\n\n                    yield f\"\\n\\n‚ö†Ô∏è **Note**: Response was truncated due to: {incomplete_reason}\"\n                    if incomplete_reason == \"max_output_tokens\":\n                        yield f\"\\nConsider increasing MAX_OUTPUT_TOKENS (currently set to {self.valves.MAX_OUTPUT_TOKENS})\"\n\n                # Extract and display tools used\n                tools_used = []\n                output_items = response_data.get(\"output\", [])\n                for item in output_items:\n                    if isinstance(item, dict) and item.get(\"type\") in [\n                        \"file_search\",\n                        \"function\",\n                    ]:\n                        tools_used.append(item[\"type\"])\n\n                if tools_used:\n                    yield f\"\\n\\nüîß **Tools used**: {', '.join(set(tools_used))}\"\n\n                # Calculate costs and update conversation totals\n                usage_data = response_data.get(\"usage\", {})\n                if usage_data:\n                    # Calculate current message cost\n                    pricing = {\n                        \"o3-pro\": {\"input\": 20, \"output\": 80},\n                        \"o1-pro\": {\"input\": 150, \"output\": 600},\n                    }\n\n                    current_cost = 0.0\n                    if model_id in pricing:\n                        input_tokens = usage_data.get(\"input_tokens\", 0)\n                        output_tokens = usage_data.get(\"output_tokens\", 0)\n                        reasoning_tokens = 0\n                        if \"output_tokens_details\" in usage_data:\n                            reasoning_tokens = usage_data[\"output_tokens_details\"].get(\n                                \"reasoning_tokens\", 0\n                            )\n\n                        input_cost = (input_tokens / 1_000_000) * pricing[model_id][\n                            \"input\"\n                        ]\n                        output_cost = (output_tokens / 1_000_000) * pricing[model_id][\n                            \"output\"\n                        ]\n                        reasoning_cost = 0\n                        if reasoning_tokens > 0 and model_id == \"o3-pro\":\n                            reasoning_cost = (reasoning_tokens / 1_000_000) * pricing[\n                                model_id\n                            ][\"output\"]\n\n                        current_cost = input_cost + output_cost + reasoning_cost\n\n                        # Update conversation totals BEFORE displaying stats\n                        self._update_conversation_cost(\n                            conv_id,\n                            current_cost,\n                            {\n                                \"input\": input_tokens,\n                                \"output\": output_tokens,\n                                \"reasoning\": reasoning_tokens,\n                            },\n                        )\n\n                    # Display token statistics (will show updated totals)\n                    stats = self._format_token_stats(\n                        usage_data,\n                        model_id,\n                        status,\n                        incomplete_reason,\n                        conv_id,\n                        current_cost,\n                    )\n                    if stats:\n                        yield stats\n                else:\n                    if self.valves.SHOW_TOKEN_STATS:\n                        yield \"\\n\\n---\\nüìä **Token Usage**: Not available in response\\n---\"\n\n        except httpx.TimeoutException:\n            yield f\"Error: Request timed out after {self.valves.TIMEOUT_SECONDS} seconds.\"\n        except Exception as e:\n            self.logger.error(f\"Pipeline error: {e}\", exc_info=True)\n            yield f\"Error: {str(e)}\"\n            return"
    },
    "info": {
      "body": "# OpenAI o1-pro & o3-pro Integration for Open Web UI\n\nAccess OpenAI's most advanced reasoning models with comprehensive token usage tracking and cost analytics directly in Open Web UI.\n\n## üåü Features\n\n- **Premium Models**: Access to o1-pro and o3-pro reasoning models\n- **Cost Tracking**: Detailed per-message and cumulative conversation costs\n- **Token Analytics**: Track input, output, and reasoning tokens separately\n- **Multi-Key Support**: Round-robin load balancing across multiple API keys\n- **Smart Conversation Tracking**: Maintains cost history throughout conversations\n- **Configurable Parameters**: Adjust reasoning effort, output length, and more\n\n## üìã Prerequisites\n\n- Open Web UI installation\n- OpenAI API key(s) with access to o1-pro and/or o3-pro models\n- Basic understanding of token-based pricing\n\n## üöÄ Quick Start\n\n1. Install the function in Open Web UI\n2. Add your OpenAI API key(s) in settings\n3. Select \"OpenAI: o1-pro\" or \"OpenAI: o3-pro\" from the model dropdown\n4. Start chatting with cost tracking enabled\n\n## ‚öôÔ∏è Configuration\n\n| Setting | Description | Default |\n|---------|-------------|---------|\n| **API_KEYS** | OpenAI API key(s), comma-separated | *(required)* |\n| **THINKING_EFFORT** | Reasoning intensity: `low`, `medium`, `high` | `medium` |\n| **SHOW_TOKEN_STATS** | Display usage after responses | `True` |\n| **SHOW_CUMULATIVE_COST** | Show conversation totals | `True` |\n| **MAX_OUTPUT_TOKENS** | Maximum response length | `3200` |\n| **TIMEOUT_SECONDS** | Request timeout | `600` |\n\n## üí∞ Model Pricing\n\n| Model | Input | Output | Best For |\n|-------|-------|--------|----------|\n| **o1-pro** | $150/M | $600/M | Complex logic, coding, math |\n| **o3-pro** | $20/M | $80/M | Research, current events, general use |\n\n## üìä Cost Tracking Example\n\n```\nüìä Token Usage (This Message):\n- Input: 1,234 tokens\n- Reasoning: 567 tokens\n- Output: 890 tokens\n- Total: 2,691 tokens\n- Cost for this message: $0.1234\n\nüí∞ Conversation Totals:\n- Messages: 3\n- Total Input: 3,456 tokens\n- Total Output: 2,345 tokens\n- Total Cost: $0.3456\n```\n\n## üéØ Model Selection Guide\n\n**Choose o1-pro when you need:**\n- Advanced mathematical problem solving\n- Complex code generation or debugging\n- Multi-step logical reasoning\n- Maximum reasoning capability\n\n**Choose o3-pro when you need:**\n- Cost-effective advanced reasoning\n- General purpose queries\n- Research and analysis tasks\n\n## ‚ö° Tips for Cost Optimization\n\n1. Use `low` thinking effort for simple queries\n2. Set appropriate `MAX_OUTPUT_TOKENS` limits\n3. Use o3-pro instead of o1-pro when possible\n4. Monitor cumulative costs during long conversations\n\n## üîß Troubleshooting\n\n**No API keys configured**\n- Add your OpenAI API key in function settings\n\n**High costs**\n- Lower `THINKING_EFFORT` setting\n- Reduce `MAX_OUTPUT_TOKENS`\n- Switch from o1-pro to o3-pro\n\n**Timeout errors**\n- Increase `TIMEOUT_SECONDS`\n- Simplify complex queries\n\n## üìù Notes\n\n- Responses appear all at once (no streaming) to ensure accurate token counting\n- Conversation costs reset when starting a new chat\n- Web search is only available for o3-pro model\n- Pricing subject to change per OpenAI's policies\n\n## ü§ù Support\n\nFor issues or suggestions, please [reach out to me](mailto:karanb192@gmail.com).\n\n---\n\n**Version**: 1.0.0  \n**License**: MIT License\n**Source Code**: [Github](https://github.com/karanb192/openwebui-o1o3-pro-plugin)  \n**Author**: Karan Bansal ¬∑ [@karanb192](https://github.com/karanb192)"
    },
    "downloads": 21364,
    "upvotes": 0,
    "downvotes": 0,
    "updatedAt": 1750602970,
    "createdAt": 1749826129,
    "user": {
      "id": "34ef1096-ef7f-40f7-993d-dbc9bdcab631",
      "username": "karanb192",
      "name": "",
      "createdAt": 1739347139,
      "role": null,
      "verified": false
    }
  }
]
